{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Model Checking\n",
    "\n",
    "Run a sample MNIST image through the saved model and observe the outputs.\n",
    "Format the predicted outputs to make it consumable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(jit_model_path):\n",
    "    model = None\n",
    "    try:\n",
    "        model = torch.jit.load(jit_model_path)\n",
    "    except RuntimeError:\n",
    "        print(f\"Unrecognized model format. Expecting model in TorchScript format. \"\n",
    "              f\"{jit_model_path}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=SimpleCNN\n",
      "  (conv1): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (conv2): RecursiveScriptModule(original_name=Conv2d)\n",
      "  (dropout1): RecursiveScriptModule(original_name=Dropout)\n",
      "  (dropout2): RecursiveScriptModule(original_name=Dropout)\n",
      "  (fc1): RecursiveScriptModule(original_name=Linear)\n",
      "  (fc2): RecursiveScriptModule(original_name=Linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL_ARTIFACT = Path(\"./artifacts/SimpleCNN_mnist_jit.pt\")\n",
    "model = load_model(MODEL_ARTIFACT)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def get_sample_image():\n",
    "    SAMPLE_IMAGES_DIR = Path(\"dataset/MNIST/sample_images\")\n",
    "    sample_images = os.listdir(SAMPLE_IMAGES_DIR)\n",
    "    sample_image = SAMPLE_IMAGES_DIR / random.choice(sample_images)\n",
    "\n",
    "    return sample_image\n",
    "\n",
    "def show_image_stats(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    print(f\"Image: {image_path.name}\", end=\"  \")\n",
    "    print(f\"Format: {img.format}  Size: {img.size}  Mode: {img.mode}\")\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "def tensorify_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    tensor_img = ToTensor()(img)\n",
    "    norm_img = Normalize((0.1307,), (0.3081,))(tensor_img)\n",
    "    return norm_img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image.as_posix()[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: image_43214_1.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAlElEQVR4nGNgGOzglDduOfXvF5lwSsp++scC56Ar4+dA4qBLcrIwSOE0dvo/JGPRAOP1f5sYcUk2/PungUuO5cK/x5y4JAP//wvH6Zx9/x7z4ZLj//FvNjIfxZ9+bAz3cUoqMnzajctUxh3/5uB0jua/f4koAsjG6jI82YVTUpth3lOckt++zWDAKXnksghO91ATAAAZKidcxbOjXQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 1  Predicted: 7\n",
      "Image: image_31386_1.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAgklEQVR4nGNgGGCQ/+8KLzKfCZlj/F9THqdOv79/9+HUyc3AkItT56K/K1EUo3AYGd/9wyn5//90BpyS6IBKkoyM9rgl//93ZcVpjN/fv+I4dT4k20EMjIzFOCW/f/7/H7c5M//ec2Aw1sUuqfv2768LHxfi0Gr7+d/fTj58jqQ1AADBUSSdfTCgEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 1  Predicted: 7\n",
      "Image: image_45146_0.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBklEQVR4nGNgGEqAEYnNqcZgYv9/7cPXzzDVGR798+fP3z9//tx1k0SX63j558+fP39f7/zz5+8pF1S50F9/n2eVdp48x+649++f66LIcm5//27WgnE2f/17mRfJLbP/PNdCcBf++SOG4Pn9+WOGZI7jqz/ZuJQyMOz4s5mBgYGJgYGBgUEsmGHNG1QHPoCzTv39roUsk/f3rzic8/fPdmQ5y9sfCjiwS0pV/f0zHUkpsqTUtD9/1yD5kmHKn29QO7k73vz508CBJMfQ8Pfv5TAJCQmn/f///j1gj+puzkPQCPnz9361IAMa0Jlx4c/fP38ON7kixxY8sgVlGP8z3PmOrovuAAC5lXiMCHBTbAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 0  Predicted: 7\n",
      "Image: image_10590_7.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGM6AlxfGYkSVMHY1ZFBlvLVy608MPUFHv/398+fP3z9/TqPqlAo3shfkZDh0lmGumlENAwtcB7us7Ky/f//+PlEIsc7t71+4XPzuP3+e/Hk50Rumdvef6XDJ/X/+/JG3UIXzS//8kWFgYGBgYGFgYJhwRu2ZyAmEu7wYbn3G4U2uzf/vSeKQY8j7cy8dl5zuhz+FuOR4D/2ZzotDTurM30O45Bge/r0mjENKeM23Q9q47TuF08xedPuYkDmfM1FDhgWJPf/rVVyGkgoAN5hLgLV3z2AAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 7  Predicted: 7\n",
      "Image: image_8610_1.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAYklEQVR4nGNgGHAg/6KHHaek4SdxJB4TqqTyv5e4JW1QeGiSDLSR1MGjVPrrRdw6LTj3ke2gA3gknx3HLanx6hVuSXM8DhD6gWIlqk4mtvW4JdEBiuTfbzx4lPadw2cQXQAAaaoSwXF1O9IAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 1  Predicted: 7\n",
      "Image: image_51853_4.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBUlEQVR4nGNgGKIgTRLODPs7gYGBgYEJLqBfKw5jmsX//4KqcesfAyiL8+SfdyIochLX36lDWFyz/xw2QdWo/GILhKE2+89rNzTnhPyZCGHM/fPaHU1O7u0fGwYGBgYG21dv0PUx9Pw5wcnAwMDAMPHPXHQ5sc9/vh/Kzc1lLfl0m4PBVNUFSY5t/x8I+PT3z++PH799/SHGwMDAwMLAwMDA8Ou0FYTBzfSPkefv7CeXXjEwMDAwQvU6azJYaijwMzC8YFi/fx26rQwMDAzFf17HYBNnYGBgEHzwJxeXHEPRn4ksuOSmfHwgiFPjwQd6OOWSb2TglNP5tgWnHIPhH0fckiQDAFLtVWfhYrhyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 4  Predicted: 7\n",
      "Image: image_34571_4.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nGNgGMRg0nUY66gjupzrp2cSDAwMDAyMhb9d0SWb/l2DMIz//VuCJqfz5N85CGvNv39+qHJsx//902NgYGBgMPz676QIquSif/82MjEwMDAwVP37J4tm6p1/X7wYGBgYGHRf/DvChipn8v7fSgir+d9zLlQ5luP//hkwMDAwMIg/+zcDzVDuf//+/f716+q0gmv/rvJhkYSBfjQ5Bo6XcLm7CgjLINQPQz4GdSXun7oxDCseoOuEAb9/79D9CAdcJ/5NROIyoUiGmTGcw6VR7Pa/Nxy4JDX//StG5jOhye/HJ0kPAABEcl+6KXcR1wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 4  Predicted: 7\n",
      "Image: image_36187_2.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDElEQVR4nL3SK0hDURzH8Z9eLk6dMNSg85HMwsr1nQTBZlwUbQOtZpvFdrOzGIYIYhIcBsFgsCyI1zTEsaLoDDrDdxi2e3ZfVX/lHM7n/DlP6f/T0+3a2W3puNqKT+qdPHgAoDgVx038nKWjNlgBmi/fACUrgiMAN/3rdwCzEey7ArYmNP4EeBl/I+3m51aSVVO9UJVmUpHS1MbR654k3QO78Q3nxnx04yhJmvsAFsJrmjhDUs1LLnQagcJIRssA00m0nC8D8FjMhWFgx/M+zf2+zQfILj0TyrtjbLEeGD9xG8C+wS8fTg8Lw5Yy53BpsNUmN915LHu1idT5JmtLki4qdH+IvXKdfNg/zy8/F6ZS5U5IcQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 2  Predicted: 7\n",
      "Image: image_9470_7.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjUlEQVR4nGNgGH6g4+/5lh//P7UYYZOs+fPnz5+/f/4810SIscAYh+8IvmdgZFIUlb+OTa8GAwOD5J8/HggRJgTzBoZyJnSBT69xulnyzxncOvXwGeuF385jOK3k/fs3BInLgiLJ/R+PsTkMr/C4luHTA5ySsgwrcbqH4e9XVzyST/A4CA2QIIk7eDABAPipKCebWy4kAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 7  Predicted: 7\n",
      "Image: image_1394_1.png  Format: PNG  Size: (28, 28)  Mode: L\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAdklEQVR4nGNgGGBg/KkFhc+EzNHjcuXBKcnAYLwDp7EKb/98xG3p0z9fpHAby8Aij1vnlb9ncesUZxDHLSmIwkO3E9UByJwkCQam37gk//9n+NeLx9ife/FI/r5OrIOoJ8mqhUsl5/M/n7hwmsO1w9sEnz10AAB6PBnsBr3lEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual 1  Predicted: 7\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sample_image = get_sample_image()\n",
    "    show_image_stats(sample_image)\n",
    "    img_tensor = tensorify_image(sample_image)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "    pred = output.argmax(dim=1, keepdim=True).item()\n",
    "    print(f\"Actual {sample_image.as_posix()[-5]}  Predicted: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(img_tensor, torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./dataset\", train=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(\"./dataset\", train=False, transform=transform)\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for data, target in mnist_train_loader:\n",
    "    break\n",
    "data.shape\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
